---
title: "STATS 506 group project"
author: "Group 13 (Jinwen Cao, Olivia Hackworth, Xian Zhang)"
date: "Nov.26 2018"
output: html_document
---

The purpose of our group is to compare the performance between Probit Regression and Logistic Regression in ¡°Communities and Crime Data Set¡± with Stata, R and Python. 

<font size="4">Introduction to Probit Regression and Logistic Regression</font><br /> 

Probit and Logistic model are both used to model dichotomous or binary outcome variables. The cumulative distribution function of the logistic distribution is modeled as a linear combination of the predictors in probit model. The logit model uses the cumulative distribution function of the logistic distribution. Both functions will take any number and rescale it to fall between 0 and 1. 

Both methods will yield similar (though not identical) inferences. Logistic model is more popular in health sciences like epidemiology partly because coefficients can be interpreted in terms of odds ratios. Probit model can be generalized to account for non-constant error variances in more advanced econometric settings (known as heteroskedastic probit models) and hence are used in some contexts by economists and political scientists. 


<font size="4">Data we used here</font><br /> 

The dataset we used is ¡°Communities and Crime Data Set¡± from UCI Machine Learning Repository. The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR.

We chose ViolentCrimesPerPop as response variable and Population, perCapInc, PctPopUnderPov, pctUrban as predictor variables. Our task is to figure out what is more likely to cause high violent crime rate based on their personal information. The probit and logistic regression are carried out to handle this challenge.


```{r, include = FALSE}
library(aod)
library(ggplot2)
library(dplyr)
library(ResourceSelection)
```
<font size="5"> 1) Loading and training data</font><br />

We selected the "Population, perCapInc, PctPopUnderPov, pctUrban" as predictor variables and deleted the missing value.

```{r}
communities.data <- read.csv("~/communities.data.txt", header=FALSE)
df3<-select(communities.data,c(6,17,26,34,128))
df3<-na.omit(df3)
a=rep(0,1994)
for(i in 1:1994){
  if(df3$V128[i]>0.5){
    a[i]=1
  }
}
df3<-cbind(df3,a)
head(df3)
```

<font size="5"> 2) Probit model</font><br />

The code below estimates a probit regression model using the glm (generalized linear model) function.

```{r}
myprobit <- glm(a ~ V6 + V17 + V26 +V34, family = binomial(link = "probit"), data = df3)
summary(myprobit)
```

The deviance residuals are a measure of model fit. This part of output shows the distribution of the deviance residuals for individual cases used in the model. The next part of the output shows the coefficients, their standard errors, the z-statistic and the associated p-values.

From the result we can see that "Population, PctPopUnderPov, pctUrban" are statistically significant while "perCapInc" doesn't have good performance. 

Below the table of coefficients are fit indices, including the null and deviance residuals and the AIC. Later we show an example of how you can use these values to help assess model fit. In addition, we can obtain confidence intervals for the coefficient estimates that is based on the standard error and the normal assumption.

```{r}
confint(myprobit)
```

We can test for an overall effect of "Population, PctPopUnderPov, pctUrban, perCapInc" using the wald.test function.

```{r}
wald.test(b = coef(myprobit), Sigma = vcov(myprobit), Terms = 2:5)
```

The chi-squared test statistic of 322.7 with 4 degrees of freedom is associated with a p-value of less than 0.001 indicating that the overall effect of rank is statistically significant. Then we use testing data set and predicted probabilities to help us understand the model. 

```{r}
df3[, c("p", "se")] <- predict(myprobit, df3, type = "response", se.fit = TRUE)[-3]
with(myprobit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

We can also see measures of how well our model fits. The chi-square of 305.8052 with 3 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits significantly better than an empty model.

Then we took the Hosmer-Lemeshow test to compare the result of probit model with logistic model. 

```{r}
hoslem.test(myprobit$y, fitted(myprobit), g=5)
```

The group number is set as 5 because this will be easy to compare the results with other language. The p-value tells us that our model is a good fit. 

<font size="5"> 3) Logistic model</font><br />

Just like the probit regression, the code below estimates a probit regression model using the glm (generalized linear model) function.

```{r}
mylogit <- glm(a ~ V6 + V17 + V26 +V34, data = df3, family = "binomial")
summary(mylogit)
confint.default(mylogit)
```

We can see that Logistic model has similar result as the Probit model. All the predictors are statistically significant except for perCapInc. (The p-value for perCapInc in Logistic model is lower than in Probit model)

```{r}
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 2:5)
```

The chi-squared test statistic of 285.1 ( a little bit lower than the one in probit regression) with 4 degrees of freedom is associated with a p-value of less than 0.001 indicating that the overall effect of rank is statistically significant. 

```{r}
 hoslem.test(mylogit$y, fitted(mylogit), g=5)
```

From the H-L test in Logistic model, we can see that the Logistic model also has a good fit. 














